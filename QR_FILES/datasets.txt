# File: app/api/endpoints/datasets.py
import csv
import io
import json
from typing import List, Optional
from uuid import UUID

from fastapi import APIRouter, Depends, File, Form, HTTPException, UploadFile, status
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.repositories.base import BaseRepository
from app.db.session import get_db
from app.models.orm.models import Dataset, DatasetType, User
from app.schema.dataset_schema import (
    DatasetCreate, DatasetResponse, DatasetUpdate, DatasetUpload
)
from app.services.auth import get_current_active_user
from app.services.storage import get_storage_service

router = APIRouter()


@router.post("/", response_model=DatasetResponse)
async def create_dataset(
        name: str = Form(...),
        description: Optional[str] = Form(None),
        type: DatasetType = Form(...),
        file: UploadFile = File(...),
        is_public: bool = Form(False),
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db)
):
    """
    Create a new dataset with file upload.
    """
    # Get storage service
    storage_service = get_storage_service()

    # Upload file
    file_path = await storage_service.upload_file(file, "datasets")

    # Analyze file and get metadata
    meta_info, row_count, schema = await analyze_file(file, type)

    # Create dataset
    dataset_data = DatasetCreate(
        name=name,
        description=description,
        type=type,
        file_path=file_path,
        schema=schema,
        meta_info=meta_info,
        row_count=row_count,
        is_public=is_public
    )

    # Create dataset in DB
    dataset_repo = BaseRepository(Dataset, db)
    dataset_dict = dataset_data.model_dump()
    dataset_dict["owner_id"] = current_user.id

    dataset = await dataset_repo.create(dataset_dict)
    return dataset


@router.get("/", response_model=List[DatasetResponse])
async def list_datasets(
        skip: int = 0,
        limit: int = 100,
        type: Optional[DatasetType] = None,
        is_public: Optional[bool] = None,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db)
):
    """
    List datasets with optional filtering.
    """
    filters = {}

    # Add filters if provided
    if type:
        filters["type"] = type
    if is_public is not None:
        filters["is_public"] = is_public

    # Get datasets owned by current user or public datasets
    dataset_repo = BaseRepository(Dataset, db)

    if current_user.role.value == "admin":
        # Admins can see all datasets
        datasets = await dataset_repo.get_multi(skip=skip, limit=limit, filters=filters)
    else:
        # Regular users can see their own datasets and public datasets
        datasets_owned = await dataset_repo.get_multi(
            skip=0, limit=None, filters={"owner_id": current_user.id, **filters}
        )

        # If is_public filter is explicitly set to False, don't fetch public datasets
        if is_public is False:
            return datasets_owned

        # Get public datasets not owned by the user
        public_filters = {"is_public": True, **filters}
        datasets_public = await dataset_repo.get_multi(
            skip=0, limit=None, filters=public_filters
        )

        # Combine and paginate manually
        all_datasets = datasets_owned + [
            d for d in datasets_public if d.owner_id != current_user.id
        ]

        # Apply pagination
        datasets = all_datasets[skip:skip + limit]

    return datasets


@router.get("/{dataset_id}", response_model=DatasetResponse)
async def get_dataset(
        dataset_id: UUID,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db)
):
    """
    Get dataset by ID.
    """
    dataset_repo = BaseRepository(Dataset, db)
    dataset = await dataset_repo.get(dataset_id)

    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Dataset with ID {dataset_id} not found"
        )

    # Check if user has permission to view this dataset
    if (
            dataset.owner_id != current_user.id
            and not dataset.is_public
            and current_user.role.value != "admin"
    ):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions"
        )

    return dataset


@router.put("/{dataset_id}", response_model=DatasetResponse)
async def update_dataset(
        dataset_id: UUID,
        dataset_data: DatasetUpdate,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db)
):
    """
    Update dataset by ID.
    """
    dataset_repo = BaseRepository(Dataset, db)
    dataset = await dataset_repo.get(dataset_id)

    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Dataset with ID {dataset_id} not found"
        )

    # Check if user has permission to update this dataset
    if dataset.owner_id != current_user.id and current_user.role.value != "admin":
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions"
        )

    # Update the dataset
    update_data = {
        k: v for k, v in dataset_data.model_dump().items() if v is not None
    }

    if not update_data:
        return dataset

    updated_dataset = await dataset_repo.update(dataset_id, update_data)
    return updated_dataset


@router.delete("/{dataset_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_dataset(
        dataset_id: UUID,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db)
):
    """
    Delete dataset by ID.
    """
    dataset_repo = BaseRepository(Dataset, db)
    dataset = await dataset_repo.get(dataset_id)

    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Dataset with ID {dataset_id} not found"
        )

    # Check if user has permission to delete this dataset
    if dataset.owner_id != current_user.id and current_user.role.value != "admin":
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions"
        )

    # Delete the dataset file
    storage_service = get_storage_service()
    await storage_service.delete_file(dataset.file_path)

    # Delete the dataset from the database
    success = await dataset_repo.delete(dataset_id)
    if not success:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to delete dataset"
        )


@router.get("/{dataset_id}/preview", response_model=List[dict])
async def preview_dataset(
        dataset_id: UUID,
        limit: int = 10,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db)
):
    """
    Preview dataset content.
    """
    dataset_repo = BaseRepository(Dataset, db)
    dataset = await dataset_repo.get(dataset_id)

    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Dataset with ID {dataset_id} not found"
        )

    # Check if user has permission to view this dataset
    if (
            dataset.owner_id != current_user.id
            and not dataset.is_public
            and current_user.role.value != "admin"
    ):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions"
        )

    # Get storage service
    storage_service = get_storage_service()

    # Read dataset file
    try:
        file_content = await storage_service.read_file(dataset.file_path)

        # Parse dataset based on type
        if dataset.type.value.endswith("json"):
            data = json.loads(file_content)
            return data[:limit] if isinstance(data, list) else [data]

        elif dataset.type.value.endswith("csv"):
            csv_data = []
            csv_file = io.StringIO(file_content)
            reader = csv.DictReader(csv_file)

            for i, row in enumerate(reader):
                if i >= limit:
                    break
                csv_data.append(dict(row))

            return csv_data

        else:
            # Return raw text for non-structured data
            return [{"content": file_content[:1000] + "..." if len(file_content) > 1000 else file_content}]

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error reading dataset file: {str(e)}"
        )


async def analyze_file(file: UploadFile, dataset_type: DatasetType) -> tuple:
    """
    Analyze file and get metadata.

    Args:
        file: Uploaded file
        dataset_type: Dataset type

    Returns:
        tuple: (metadata, row_count, schema)
    """
    # Reset file position
    await file.seek(0)
    content = await file.read()

    try:
        metadata = {
            "filename": file.filename,
            "content_type": file.content_type,
            "size": len(content)
        }

        # Parse based on file type
        if file.content_type == "application/json" or dataset_type.value.endswith("json"):
            # Parse JSON
            data = json.loads(content.decode("utf-8"))

            if isinstance(data, list):
                row_count = len(data)
                # Infer schema from first item if it's a list
                schema = {"properties": {}} if not data else {
                    "properties": {
                        k: {"type": type(v).__name__} for k, v in data[0].items()
                    }
                }
            else:
                row_count = 1
                schema = {"properties": {
                    k: {"type": type(v).__name__} for k, v in data.items()
                }}

        elif file.content_type == "text/csv" or dataset_type.value.endswith("csv"):
            # Parse CSV
            csv_text = content.decode("utf-8")
            csv_file = io.StringIO(csv_text)
            reader = csv.reader(csv_file)

            # Get header and first row to infer schema
            header = next(reader, [])
            first_row = next(reader, [])

            schema = {"properties": {}}
            if header and first_row:
                for i, col in enumerate(header):
                    if i < len(first_row):
                        # Try to infer type
                        val = first_row[i]
                        try:
                            int(val)
                            schema["properties"][col] = {"type": "integer"}
                        except ValueError:
                            try:
                                float(val)
                                schema["properties"][col] = {"type": "number"}
                            except ValueError:
                                schema["properties"][col] = {"type": "string"}
                    else:
                        schema["properties"][col] = {"type": "string"}

            # Count rows
            csv_file.seek(0)
            row_count = sum(1 for _ in csv.reader(csv_file)) - 1  # Subtract header

        else:
            # Text or other format
            row_count = 1
            schema = {"type": "string"}

        return metadata, row_count, schema

    except Exception as e:
        # If analysis fails, return basic info
        return {
            "filename": file.filename,
            "content_type": file.content_type,
            "size": len(content),
            "error": str(e)
        }, 0, {}